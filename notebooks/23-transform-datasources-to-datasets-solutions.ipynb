{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Transforming DataSources into Datasets\n",
    "“It is a capital mistake to theorize before one has data.” Sherlock Holmes, “A Study in Scarlett” (Arthur Conan Doyle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a `DataSource` into a `Dataset`\n",
    "\n",
    "### The Dataset object\n",
    "\n",
    "A Dataset is the fundamental object we use for making the \"munge\" part of our data flow reproducible\n",
    "\n",
    "What's a Dataset object? It's a scikit-learn-style Bunch, containing:\n",
    "\n",
    "    data: the processed data\n",
    "    target: (optional) target vector (for supervised learning problems)\n",
    "    metadata: Data about the data\n",
    "\n",
    "Under the hood, this is esentially a dictionary, with a little bit of magic to make it nicer to work with.\n",
    "\n",
    "How do we turn raw data sources into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw source files, that is, a **parse function**, and\n",
    "2. package this **parse function** according to a very simple API\n",
    "\n",
    "\n",
    "First, let's grab the `DataSource` we created in the last notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a `DataSource` from the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src import paths\n",
    "from src.data import DataSource\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = dsrc.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a `Dataset` object\n",
    "\n",
    "A `Dataset` is the fundamental object we use for making the \"munge\" part of our data flow reproducible\n",
    "\n",
    "What's a Dataset object? It's a scikit-learn-style Bunch, containing:\n",
    "\n",
    "    data: the processed data\n",
    "    target: (optional) target vector (for supervised learning problems)\n",
    "    metadata: Data about the data\n",
    "\n",
    "Under the hood, this is esentially a dictionary, with a little bit of magic to make it nicer to work with.\n",
    "\n",
    "Lucky for us, we can pick up from where we were at with our `DataSource`, and simply use the a parse function to turn it into a `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "The next step is to write the importer that actually processes the data we will be using for this dataset.\n",
    "\n",
    "The important things to generate are `data` and `target` entries. A `metadata` is optional, but recommended if you want to save additional information about the dataset.\n",
    "\n",
    "### `parse_function` Template\n",
    "A **parse function** is a function that conforms to a very simple API: given some input, it returns a triple\n",
    "\n",
    "```(data, target, additional_metadata)```\n",
    "\n",
    "\n",
    "where `data` and `target` are in a format ingestible by, say, an sklearn pipeline.\n",
    "`additional_metadata` is a dictionary of key-value pairs that will be added to any existing metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Processing lvq-pak data\n",
    "Let's convert the lvq-pak data (introduced in the last section) into into `data` and `target` vectors.\n",
    "\n",
    "#### Some initial exploration of lvq-pak datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir/lvq_pak-3.1  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these datafiles look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $datafile_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `datafile_train` (`ex1.dat`) appears to consists of:\n",
    "* the number of data columns, followed by\n",
    "* a comment line, then\n",
    "* space-delimited data\n",
    "\n",
    "**Wait!** There's a gotcha here. Look at the last entry in each row. That's the data label. In the last row, however, we see that `#` is used as a data label (easily confused for a comment). Be careful handling this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $datafile_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `datafile_test` (`ex2.dat`) is similar, but has no comment header.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initial exploration of F-MNIST datafiles\n",
    "\n",
    "Take a look at the F-MNIST datafiles. Plot one of the images. \n",
    "\n",
    "**Hint:** See https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py to get an idea for how to read in the labels and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fmnist DataSource\n",
    "fmnist = DataSource.from_name('fmnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_unpack_dir = fmnist.unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $fmnist_unpack_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $fmnist_unpack_dir/fmnist.readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the F-MNIST repo, we see that they actually have a parser function that shows you how to use this data! https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing labels aka. target\n",
    "with open(fmnist_unpack_dir / 'train-labels-idx1-ubyte', 'rb') as labels:\n",
    "    target = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing images aka. data\n",
    "with open(fmnist_unpack_dir / 'train-images-idx3-ubyte', 'rb')as images:\n",
    "    data = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's take a look at one of the images (and to check that we're on track!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not added yet, you'll need to add matplolib to your environment.yml file and make requirements!\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot an image!\n",
    "plt.imshow(data[0].reshape(28, 28), cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing lvq-pak data files\n",
    "\n",
    "\n",
    "Recall that we want to create a `parse_function` with the following API: given some input, it returns a triple\n",
    "\n",
    "```(data, target, additional_metadata)```\n",
    "\n",
    "\n",
    "where `data` and `target` are in a format ingestible by, say, an sklearn pipeline.\n",
    "`additional_metadata` is a dictionary of key-value pairs that will be added to any existing metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True, metadata=None):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also\n",
    "    used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: list-like, int or callable, optional\n",
    "        list of rows to skip when reading the file. See `pandas.read_csv`\n",
    "        entry on `skiprows` for more\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_csv(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `read_space_delimited` can be imported from `src.data.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target, metadata = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write a parsing function for F-MNIST\n",
    "\n",
    "Write a function that takes as input the F-MNIST data path, and returns a `(data, target, metadata)` triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fmnist(data_path, kind='train', metadata=None):\n",
    "    \"\"\"\n",
    "    Read fmnist data files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path: path\n",
    "        base directory to look for the files in\n",
    "    kind: one of 'train' and 'test'\n",
    "        whether to parse the training or test datasets\n",
    "    metadata: dict\n",
    "        metadata to add to the process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (data, target, metadata)\n",
    "    \"\"\"\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    \n",
    "    if kind == 'train':\n",
    "        name_kind = kind\n",
    "    elif kind == 'test':\n",
    "        name_kind = 't10k'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kind:{kind}\")\n",
    "\n",
    "    # parsing labels aka. target\n",
    "    with open(data_path / f'{name_kind}-labels-idx1-ubyte', 'rb') as labels:\n",
    "        target = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
    "        \n",
    "    # parsing images aka. data\n",
    "    with open(data_path / f'{name_kind}-images-idx3-ubyte', 'rb')as images:\n",
    "        data = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)\n",
    "        \n",
    "    return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out\n",
    "data, target, metadata = read_fmnist(fmnist_unpack_dir, kind='train')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target, metadata = read_fmnist(fmnist_unpack_dir, kind='test')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a process function\n",
    "\n",
    "We could be done here, but let's go a little further and allow the parsing function to return either `train`, `test` or `all` data. In other words, let's create a processing function: `process_lvq_pak` that takes a `kind` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(*, unpack_dir, kind='all', extract_dir='lvq_pak-3.1', metadata=None):\n",
    "    \"\"\"\n",
    "    Parse LVQ-PAK datafiles into usable numpy arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    unpack_dir: path\n",
    "        path to unpacked tarfile\n",
    "    extract_dir: string\n",
    "        name of directory in the unpacked tarfile containing\n",
    "        the raw data files\n",
    "    kind: {'train', 'test', 'all'}\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple: \n",
    "       (data, target, additional_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    if unpack_dir:\n",
    "        unpack_dir = pathlib.Path(unpack_dir)\n",
    "\n",
    "    data_dir = unpack_dir / extract_dir\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex1.dat',\n",
    "                                                      skiprows=[0,1],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex2.dat',\n",
    "                                                      skiprows=[0],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_space_delimited(data_dir / 'ex1.dat', skiprows=[0,1],\n",
    "                                                        metadata=metadata)\n",
    "        data2, target2, metadata = read_space_delimited(data_dir / 'ex2.dat', skiprows=[0],\n",
    "                                                        metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data by default\n",
    "data, target, metadata = process_lvq_pak(unpack_dir=unpack_dir)\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data \n",
    "data, target, metadata = process_lvq_pak(unpack_dir=unpack_dir, kind='train')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data \n",
    "data, target, metadata = process_lvq_pak(unpack_dir=unpack_dir, kind='test')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, by adding `process_lvq_pak` to our `DataSource` object as a `parse_function`, we'll be able to reproducibly create a `Dataset` from our `DataSource`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.parse_function = partial(process_lvq_pak, unpack_dir=str(unpack_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.dataset_opts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exericse: Write a process function for F-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fmnist(*, unpack_dir, kind='all', extract_dir=None, metadata=None):\n",
    "    \"\"\"\n",
    "    Load the F-MNIST dataset \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unpack_dir: path\n",
    "        path to unpacked tarfile\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Dataset comes pre-split into training and test data.\n",
    "        Indicates which dataset to load\n",
    "    metadata: dict\n",
    "        Additional metadata fields will be added to this dict.\n",
    "        'kind': value of `kind` used to generate a subset of the data\n",
    "    '''\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple: \n",
    "       (data, target, additional_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    if unpack_dir:\n",
    "        unpack_dir = pathlib.Path(unpack_dir)\n",
    "    if extract_dir is not None:\n",
    "        data_dir = unpack_dir / extract_dir\n",
    "    else:\n",
    "        data_dir = unpack_dir\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_fmnist(data_dir, kind='train', metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_fmnist(data_dir, kind='test', metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_fmnist(data_dir, kind='train', metadata=metadata)\n",
    "        data2, target2, metadata = read_fmnist(data_dir, kind='test', metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out\n",
    "data, target, metadata = process_fmnist(unpack_dir=fmnist_unpack_dir, kind='train')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out\n",
    "data, target, metadata = process_fmnist(unpack_dir=fmnist_unpack_dir, kind='test')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out\n",
    "data, target, metadata = process_fmnist(unpack_dir=fmnist_unpack_dir, kind='all')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the process function to the dataset\n",
    "fmnist.parse_function = partial(process_fmnist, unpack_dir=str(fmnist_unpack_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.dataset_opts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process() # Use the process function to convert this DataSource to a real Dataset\n",
    "str(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write this into the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to save this to the workflow. We can just do the same as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_datasource(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_catalog, dset_catalog_file = workflow.available_datasources(keys_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_catalog['lvq-pak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `parse_lvq_pak` to the `src` module\n",
    "\n",
    "Part of making things reproducible is moving helper functions out of notebooks and into the `src` module as we go. By convention, we add custom dataset processing and generation function to `src/data/localdata.py`. \n",
    "\n",
    "### Exercise: Use the `src` module for reproducibility\n",
    "Add `process_lvq_pak` to `localdata.py`, (and add it to `__all__`) to make it visible to our dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../src/data/localdata.py\n",
    "\"\"\"\n",
    "Custom dataset processing/generation functions should be added to this file\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "from .utils import read_space_delimited\n",
    "import numpy as np\n",
    "\n",
    "__all__ = [\n",
    "    'process_lvq_pak'\n",
    "]\n",
    "\n",
    "\n",
    "def process_lvq_pak(*, unpack_dir, kind='all', extract_dir='lvq_pak-3.1', metadata=None):\n",
    "    \"\"\"\n",
    "    Parse LVQ-PAK datafiles into usable numpy arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    unpack_dir: path\n",
    "        path to unpacked tarfile\n",
    "    extract_dir: string\n",
    "        name of directory in the unpacked tarfile containing\n",
    "        the raw data files\n",
    "    kind: {'train', 'test', 'all'}\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple: \n",
    "       (data, target, additional_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    if unpack_dir:\n",
    "        unpack_dir = pathlib.Path(unpack_dir)\n",
    "\n",
    "    data_dir = unpack_dir / extract_dir\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex1.dat',\n",
    "                                                      skiprows=[0,1],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex2.dat',\n",
    "                                                      skiprows=[0],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_space_delimited(data_dir / 'ex1.dat', skiprows=[0,1],\n",
    "                                                        metadata=metadata)\n",
    "        data2, target2, metadata = read_space_delimited(data_dir / 'ex2.dat', skiprows=[0],\n",
    "                                                        metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    return data, target, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import call should now work!\n",
    "from src.data.localdata import process_lvq_pak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Use `process_lvq_pak` from `src.data.localdata` as the `parse_function`, and re-add the `DataSource` to the `workflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.parse_function = partial(process_lvq_pak, unpack_dir=str(unpack_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_datasource(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should point to the function in the src module, not the function in this notebook!\n",
    "dset_catalog, dset_catalog_file = workflow.available_datasources(keys_only=False)\n",
    "dset_catalog['lvq-pak']['load_function_module']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check things still work!\n",
    "dsrc.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Stop and check everything in using git!\n",
    "\n",
    "Use a branch, a PR via GitHub or BitBucket, and pull your changes to `src/data/localdata.py` all back into your local master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Finish turning the the F-MNIST `DataSource` into a `Dataset`\n",
    "* save your parse functions in the `src` module\n",
    "* re-add your datasource to your in the `workflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add process fmnist to the src module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../src/data/localdata.py\n",
    "\"\"\"\n",
    "Custom dataset processing/generation functions should be added to this file\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "from .utils import read_space_delimited\n",
    "import numpy as np\n",
    "\n",
    "__all__ = [\n",
    "    'process_lvq_pak',\n",
    "    'process_fmnist'\n",
    "]\n",
    "\n",
    "\n",
    "def process_lvq_pak(*, unpack_dir, kind='all', extract_dir='lvq_pak-3.1', metadata=None):\n",
    "    \"\"\"\n",
    "    Parse LVQ-PAK datafiles into usable numpy arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    unpack_dir: path\n",
    "        path to unpacked tarfile\n",
    "    extract_dir: string\n",
    "        name of directory in the unpacked tarfile containing\n",
    "        the raw data files\n",
    "    kind: {'train', 'test', 'all'}\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple: \n",
    "       (data, target, additional_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    if unpack_dir:\n",
    "        unpack_dir = pathlib.Path(unpack_dir)\n",
    "\n",
    "    data_dir = unpack_dir / extract_dir\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex1.dat',\n",
    "                                                      skiprows=[0,1],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex2.dat',\n",
    "                                                      skiprows=[0],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_space_delimited(data_dir / 'ex1.dat', skiprows=[0,1],\n",
    "                                                        metadata=metadata)\n",
    "        data2, target2, metadata = read_space_delimited(data_dir / 'ex2.dat', skiprows=[0],\n",
    "                                                        metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    return data, target, metadata\n",
    "\n",
    "def read_fmnist(data_path, kind='train', metadata=None):\n",
    "    \"\"\"\n",
    "    Helper function to read fmnist data files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path: path\n",
    "        base directory to look for the files in\n",
    "    kind: one of 'train' and 'test'\n",
    "        whether to parse the training or test datasets\n",
    "    metadata: dict\n",
    "        metadata to add to the process\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (data, target, metadata)\n",
    "    \"\"\"\n",
    "    data_path = pathlib.Path(data_path)\n",
    "    \n",
    "    if kind == 'train':\n",
    "        name_kind = kind\n",
    "    elif kind == 'test':\n",
    "        name_kind = 't10k'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown kind:{kind}\")\n",
    "\n",
    "    # parsing labels aka. target\n",
    "    with open(data_path / f'{name_kind}-labels-idx1-ubyte', 'rb') as labels:\n",
    "        target = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
    "        \n",
    "    # parsing images aka. data\n",
    "    with open(data_path / f'{name_kind}-images-idx3-ubyte', 'rb')as images:\n",
    "        data = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(len(target), 784)\n",
    "        \n",
    "    return data, target, metadata\n",
    "\n",
    "def process_fmnist(*, unpack_dir, kind='all', extract_dir=None, metadata=None):\n",
    "    \"\"\"\n",
    "    Load the F-MNIST dataset \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unpack_dir: path\n",
    "        path to unpacked tarfile\n",
    "    kind: {'train', 'test', 'all'}\n",
    "        Dataset comes pre-split into training and test data.\n",
    "        Indicates which dataset to load\n",
    "    metadata: dict\n",
    "        Additional metadata fields will be added to this dict.\n",
    "        'kind': value of `kind` used to generate a subset of the data\n",
    "    '''\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple: \n",
    "       (data, target, additional_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    if unpack_dir:\n",
    "        unpack_dir = pathlib.Path(unpack_dir)\n",
    "    if extract_dir is not None:\n",
    "        data_dir = unpack_dir / extract_dir\n",
    "    else:\n",
    "        data_dir = unpack_dir\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_fmnist(data_dir, kind='train', metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_fmnist(data_dir, kind='test', metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_fmnist(data_dir, kind='train', metadata=metadata)\n",
    "        data2, target2, metadata = read_fmnist(data_dir, kind='test', metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.localdata import process_fmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that `process_fmnist` is coming from the correct src.data.localdata\n",
    "process_fmnist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add process_fmnist as a parse function to the fmnist\n",
    "# DataSource for both train and test (based on the function in src)\n",
    "fmnist_train = DataSource.from_name('fmnist')\n",
    "fmnist_train.name = 'fmnist_train'\n",
    "\n",
    "fmnist_test = DataSource.from_name('fmnist')\n",
    "fmnist_test.name = 'fmnist_test'\n",
    "\n",
    "fmnist_train.name, fmnist_test.name, fmnist.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist.parse_function  = partial(process_fmnist, unpack_dir=str(fmnist_unpack_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test things out\n",
    "fmnist_dataset = fmnist.process(kind='train')\n",
    "fmnist_dataset.data.shape, fmnist_dataset.target.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_dataset = fmnist.process(kind='test')\n",
    "fmnist_dataset.data.shape, fmnist_dataset.target.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_dataset = fmnist.process()\n",
    "fmnist_dataset.data.shape, fmnist_dataset.target.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the fmnist DataSource to the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_datasource(fmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the fmnist DataSource `load_function_name` is pointing to the `src` module\n",
    "dset_catalog, dset_catalog_file = workflow.available_datasources(keys_only=False)\n",
    "dset_catalog['fmnist']['load_function_module'], dset_catalog['fmnist']['load_function_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check everything in using git\n",
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating the workflow\n",
    "\n",
    "What we have so far is enough to be able to load a `Dataset` from a `DataSource`. We want to go a step further and add the generation of this data to the automated workflow so that we can blow away our data and recreate it using `make` commands.\n",
    "\n",
    "Next up, we want to be able to `make data`:\n",
    "<img src=\"references/cheat_sheet.png\" alt=\"Reproducible Data Science Workflow\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq_pak = Dataset.from_datasource('lvq-pak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(lvq_pak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall: so far we have up to `make sources`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make clean_raw && make clean_interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can recover our newly created `Datasets` from our `DataSources`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_datasource('lvq-pak')\n",
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_datasource('lvq-pak', kind='train')\n",
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers: an intro to `make data`\n",
    "\n",
    "We still need to automate our the `Dataset` generation as part of our `workflow`. We'll do this using `transformers` (which we'll get into more in the next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make clean_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $paths.data_path/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $paths.data_path/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make clean_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $paths.data_path/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's encode this as a transformer from a `DataSource` to a `Dataset` as part of our automated, reproducible workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(from_datasource='lvq-pak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.get_transformer_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $paths.data_path/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make clean_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $paths.data_path/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create the F-MNIST dataset\n",
    "\n",
    "* Create an F-MNIST `Dataset` for both `train` and `test`\n",
    "* Blow it away and recreate it using `make data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(from_datasource='fmnist',\n",
    "                         datasource_opts={'kind':'train'},\n",
    "                         output_dataset='fmnist_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_transformer(from_datasource='fmnist',\n",
    "                         datasource_opts={'kind':'test'},\n",
    "                         output_dataset='fmnist_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('fmnist_train')\n",
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load('fmnist_test')\n",
    "ds.data.shape, ds.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Reproducible Datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
