{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Transforming Data Sources into Data\n",
    "“It is a capital mistake to theorize before one has data.” Sherlock Holmes, “A Study in Scarlett” (Arthur Conan Doyle).\n",
    "\n",
    "“If we have data, let’s look at data. If all we have are opinions, let’s go with mine.” – Jim Barksdale, former Netscape CEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.logging import logger\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning a `DataSource` into a `Dataset`\n",
    "How do we turn raw data sources into something useful? There are 2 steps:\n",
    "1. Write a function to extract meaningful `data` (and optionally, `target`) objects from your raw source files, ( a **parse function**, and\n",
    "2. package this **parse function** according to a very simple API\n",
    "\n",
    "\n",
    "First, let's grab the dataset we created in the last notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a `DataSet` from the Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import workflow\n",
    "from src.data import DataSource\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource.from_name('lvq-pak')    # load it from the catalog\n",
    "unpack_dir = dsrc.unpack()                # Find the location of the unpacked files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `parse_function` Template\n",
    "A **parse function** is a function that conforms to a very simple API: given some input, it returns a triple\n",
    "\n",
    "```(data, target, additional_metadata)```\n",
    "\n",
    "\n",
    "where `data` and `target` are in a format ingestible by, say, an sklearn pipeline.\n",
    "`additional_metadata` is a dictionary of key-value pairs that will be added to any existing metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Processing lvq-pak data\n",
    "Let's convert the lvq-pak data (introduced in the last section) into into `data` and `target` vectors.\n",
    "\n",
    "#### Some exploratory EDA on lvq-pak datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la $unpack_dir/lvq_pak-3.1  # Files are extracted to a subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'lvq_pak-3.1' / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'lvq_pak-3.1' / 'ex2.dat'\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these datafiles look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $datafile_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `datafile_train` (`ex1.dat`) appears to consists of:\n",
    "* the number of data columns, followed by\n",
    "* a comment line, then\n",
    "* space-delimited data\n",
    "\n",
    "**Wait!** There's a gotcha here. Look at the last entry in each row. That's the data label. In the last row, however, we see that `#` is used as a data label (easily confused for a comment). Be careful handling this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 $datafile_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `datafile_test` (`ex2.dat`) is similar, but has no comment header.\n",
    " \n",
    "#### Parsing lvq-pak data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_space_delimited(filename, skiprows=None, class_labels=True, metadata=None):\n",
    "    \"\"\"Read an space-delimited file\n",
    "    \n",
    "    Data is space-delimited. Last column is the (string) label for the data\n",
    "\n",
    "    Note: we can't use automatic comment detection, as `#` characters are also\n",
    "    used as data labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    skiprows: list-like, int or callable, optional\n",
    "        list of rows to skip when reading the file. See `pandas.read_csv`\n",
    "        entry on `skiprows` for more\n",
    "    class_labels: boolean\n",
    "        if true, the last column is treated as the class (target) label\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as fd:\n",
    "        df = pd.read_csv(fd, skiprows=skiprows, skip_blank_lines=True,\n",
    "                           comment=None, header=None, sep=' ', dtype=str)\n",
    "        # targets are last column. Data is everything else\n",
    "        if class_labels is True:\n",
    "            target = df.loc[:, df.columns[-1]].values\n",
    "            data = df.loc[:, df.columns[:-1]].values\n",
    "        else:\n",
    "            data = df.values\n",
    "            target = np.zeros(data.shape[0])\n",
    "        return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target, metadata = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could be done here, but let's go a little further and allow the parsing function to return either `train`, `test` or `all` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lvq_pak(*, unpack_dir, kind='all', extract_dir='lvq_pak-3.1', metadata=None):\n",
    "    \"\"\"\n",
    "    Parse LVQ-PAK datafiles into usable numpy arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    unpack_dir: path\n",
    "        path to unpacked tarfile\n",
    "    extract_dir: string\n",
    "        name of directory in the unpacked tarfile containing\n",
    "        the raw data files\n",
    "    kind: {'train', 'test', 'all'}\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple: \n",
    "       (data, target, additional_metadata)\n",
    "    \n",
    "    \"\"\"\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "\n",
    "    if unpack_dir:\n",
    "        unpack_dir = pathlib.Path(unpack_dir)\n",
    "\n",
    "    data_dir = unpack_dir / extract_dir\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex1.dat',\n",
    "                                                      skiprows=[0,1],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'test':\n",
    "        data, target, metadata = read_space_delimited(data_dir / 'ex2.dat',\n",
    "                                                      skiprows=[0],\n",
    "                                                      metadata=metadata)\n",
    "    elif kind == 'all':\n",
    "        data1, target1, metadata = read_space_delimited(data_dir / 'ex1.dat', skiprows=[0,1],\n",
    "                                                        metadata=metadata)\n",
    "        data2, target2, metadata = read_space_delimited(data_dir / 'ex2.dat', skiprows=[0],\n",
    "                                                        metadata=metadata)\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    return data, target, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data by default\n",
    "data, target, metadata = process_lvq_pak(unpack_dir=unpack_dir)\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data \n",
    "data, target, metadata = process_lvq_pak(unpack_dir=unpack_dir, kind='train')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data \n",
    "data, target, metadata = process_lvq_pak(unpack_dir=unpack_dir, kind='test')\n",
    "data.shape, target.shape, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.parse_function = partial(process_lvq_pak, unpack_dir=str(unpack_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc.dataset_opts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write this into the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to save this to the workflow. We can just do the same as before, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_datasource(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.available_datasources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_catalog, dset_catalog_file = workflow.available_datasources(keys_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_catalog['lvq-pak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process() # Use the load_function to convert this DataSource to a real Dataset\n",
    "str(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dsrc.process(kind=\"test\")  # Should be half the size\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Turn the F-MNIST `DataSource` into a `Dataset`\n",
    "In the last exercise, you fetched and unpacked F-MNIST data.\n",
    "Now it's time to process it into a `Dataset` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Dataset` and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tour of the Dataset Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducible Data: The Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
